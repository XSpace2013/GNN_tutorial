{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Description\n",
        "\n",
        "We train a simple GNN comprising of a user-specified number of GCN layers and hidden dimension. The [ogbn-arxiv odataset](https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv) comes from [Open Graph Benchmark](https://ogb.stanford.edu/).\n",
        "\n",
        "The goal is not to reach the state-of-the-art performance as deep GNN models, but rather illustrate how GNN is trained using the [PyG](https://pytorch-geometric.readthedocs.io/en/latest/) package."
      ],
      "metadata": {
        "id": "B16P07YpdPkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdRF5Nr3zCQy",
        "outputId": "1fbed066-4fc8-4c77-d778-c53afcc3ae00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-6a69925c-1d2b-7bf8-6d53-39cfaafd42af)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0LfvspJpN1x",
        "outputId": "f61e58ca-93e9-4627-9bd6-2dd2bffd0fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.8/dist-packages (from import_ipynb) (7.9.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (5.1.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->import_ipynb) (2.16.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->IPython->import_ipynb) (0.8.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.10.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.19.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (1.15.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat->import_ipynb) (2.6.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import_ipynb) (3.11.0)\n",
            "Installing collected packages: jedi, import_ipynb\n",
            "Successfully installed import_ipynb-0.1.4 jedi-0.18.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (4.64.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.15.0)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.3.5)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb) (2.25.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->ogb) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7047 sha256=0406cd1c91f9338456eefbb3e17e983e475314abaeb5a28c82fc2ee3732efc6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/33/c4/0ef84d7f5568c2823e3d63a6e08988852fb9e4bc822034870a\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=4d2f9ef8cdee48b4c7dbd45ea8b4908e80689b1c77ad86e28ec87613ed6cbe86\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/03/bb/7a97840eb54479b328672e15a536e49dc60da200fb21564d53\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install import_ipynb\n",
        "!pip install ogb\n",
        "!pip install GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q6oQVsXSLJW-"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import sys\n",
        "import importlib as ipb\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, ChebConv\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import GPUtil\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VFMP5RgRYnhA"
      },
      "outputs": [],
      "source": [
        "class Logger(object):\n",
        "    def __init__(self, runs, info=None):\n",
        "        self.info = info\n",
        "        self.results = [[] for _ in range(runs)]\n",
        "      \n",
        "    def pickle(self, key_save):\n",
        "        f = open(key_save, 'wb')\n",
        "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
        "        f.close()\n",
        "\n",
        "    def unpickle(self, key_save):\n",
        "        with open(key_save, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def add_result(self, run, result):\n",
        "        assert len(result) == 3\n",
        "        assert run >= 0 and run < len(self.results)\n",
        "        self.results[run].append(result)\n",
        "\n",
        "    def print_statistics(self, run=None):\n",
        "        if run is not None:\n",
        "            result = 100 * torch.tensor(self.results[run])\n",
        "            argmax = result[:, 1].argmax().item()\n",
        "            print(f'Run {run + 1:02d}:')\n",
        "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
        "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
        "            print(f'Highest Test: {result[:, 2].max():.2f}')\n",
        "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
        "            print(f'  Final Valid: {result[argmax, 1]:.2f}')\n",
        "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
        "        else:\n",
        "            result = 100 * torch.tensor(self.results)\n",
        "\n",
        "            best_results = []\n",
        "            for r in result:\n",
        "                train = r[:, 0].max().item()\n",
        "                valid = r[:, 1].max().item()\n",
        "                test = r[:, 2].max().item()\n",
        "                train2 = r[r[:, 1].argmax(), 0].item()\n",
        "                test2 = r[r[:, 1].argmax(), 2].item()\n",
        "                best_results.append((train, valid, test, train2, test2))\n",
        "\n",
        "            best_result = torch.tensor(best_results)\n",
        "\n",
        "            print(f'All runs:')\n",
        "            r = best_result[:, 0]\n",
        "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
        "            r = best_result[:, 1]\n",
        "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
        "            r = best_result[:, 2]\n",
        "            print(f'Highest Test: {r.mean():.2f} ± {r.std():.2f}')\n",
        "            r = best_result[:, 3]\n",
        "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
        "            r = best_result[:, 1]\n",
        "            print(f'  Final Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
        "            r = best_result[:, 4]\n",
        "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')\n",
        "\n",
        "\n",
        "def test(model, data_train, data, split_idx, evaluator):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_out=model(data_train.x, data_train.edge_index)\n",
        "        y_pred_train=train_out.argmax(dim=-1, keepdim=True)\n",
        "        out = model(data.x, data.adj_t)\n",
        "        y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "        train_acc = evaluator.eval({\n",
        "            'y_true': data_train.y,\n",
        "            'y_pred': y_pred_train,\n",
        "        })['acc']\n",
        "        valid_acc = evaluator.eval({\n",
        "            'y_true': data.y[split_idx['valid']],\n",
        "            'y_pred': y_pred[split_idx['valid']],\n",
        "        })['acc']\n",
        "        test_acc = evaluator.eval({\n",
        "            'y_true': data.y[split_idx['test']],\n",
        "            'y_pred': y_pred[split_idx['test']],\n",
        "        })['acc']\n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q92j2DdFsuq9"
      },
      "outputs": [],
      "source": [
        "def slide_idx(data, indices):\n",
        "    full_mat = data.adj_t.to_scipy().tocsr()\n",
        "    coo = full_mat[indices][:, indices].tocoo() # This is wrong, because it omits \"papers\" in the future\n",
        "    values = coo.data\n",
        "    sub_indices = np.vstack((coo.row, coo.col))\n",
        "    i = torch.LongTensor(sub_indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = coo.shape\n",
        "    sub_idx = torch.sparse.FloatTensor(i, v, torch.Size(shape)).coalesce().indices()\n",
        "    sub_x = data.x[indices]\n",
        "    sub_y = data.y[indices]\n",
        "    return Data(x=sub_x, y=sub_y, edge_index=sub_idx).to(device)\n",
        "\n",
        "def mem_report():\n",
        "    if device.type == 'cuda':\n",
        "        GPUs = GPUtil.getGPUs()\n",
        "        for i, gpu in enumerate(GPUs):\n",
        "            print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(\n",
        "                i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
        "    else:\n",
        "        print(\"CPU RAM Free: \"\n",
        "              + humanize.naturalsize(psutil.virtual_memory().available))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WPIh-KvoaA_m"
      },
      "outputs": [],
      "source": [
        "def train(model, data, args):\n",
        "    model.train()\n",
        "    # No batch\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.nll_loss(out, data.y.squeeze(1))\n",
        "    loss.backward()\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9PyVnBo8LWHv"
      },
      "outputs": [],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout, FC=False):\n",
        "        super(GNN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.FC = FC\n",
        "        if self.FC:\n",
        "            self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
        "        else:\n",
        "            self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            if self.FC:\n",
        "                self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
        "            else:\n",
        "                self.convs.append(\n",
        "                    GCNConv(hidden_channels, hidden_channels, cached=True))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        if self.FC:\n",
        "            self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
        "        else:\n",
        "            self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            if i > 0:\n",
        "                x = self.bns[i-1](x)\n",
        "            if self.FC:\n",
        "                x = conv(x)\n",
        "            else:\n",
        "                x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            if i == len(self.convs[:-1])-1:\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x.log_softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_mod = GNN(10,50,10,3,False)\n",
        "example_mod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h31jZdEKb24B",
        "outputId": "77d19df5-f4da-4619-b155-14b95ed2d405"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GNN(\n",
              "  (convs): ModuleList(\n",
              "    (0): GCNConv(10, 50)\n",
              "    (1): GCNConv(50, 50)\n",
              "    (2): GCNConv(50, 10)\n",
              "  )\n",
              "  (bns): ModuleList(\n",
              "    (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xUHNMsO_NKCf",
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d0c0b4-8197-4e46-e39a-76c0a659c6ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(FC=False, SVI=False, batch=1, change_ratio=False, dec_epoch=100, dropout=0.25, epochs=5, f='/root/.local/share/jupyter/runtime/kernel-39b682c3-ad09-4f3a-a3c4-7fdfa7129cb9.json', hidden_channels=512, log_steps=1, lr=0.001, lr_drop=0.98, momentum=0.95, num_layers=4, optimizer='Adam', ratio_mult=1, runs=3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch_sparse/storage.py:14: UserWarning: `layout` argument unset, using default layout \"coo\". This may lead to unexpected behaviour.\n",
            "  warnings.warn('`layout` argument unset, using default layout '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Run: 01, Epoch: 01, Loss: 4.0162, Train: 11.79%, Valid: 23.30% Test: 21.87%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Run: 01, Epoch: 02, Loss: 2.8619, Train: 25.56%, Valid: 31.64% Test: 34.01%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Run: 01, Epoch: 03, Loss: 2.2463, Train: 17.99%, Valid: 18.81% Test: 24.98%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Run: 01, Epoch: 04, Loss: 1.9026, Train: 17.14%, Valid: 18.46% Test: 24.63%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8857MB / 15360MB | Utilization  41%\n",
            "Run: 01, Epoch: 05, Loss: 1.7206, Train: 17.93%, Valid: 18.90% Test: 24.92%\n",
            "Run 01:\n",
            "Highest Train: 25.56\n",
            "Highest Valid: 31.64\n",
            "Highest Test: 34.01\n",
            "  Final Train: 25.56\n",
            "  Final Valid: 31.64\n",
            "   Final Test: 34.01\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8853MB / 15360MB | Utilization  41%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8853MB / 15360MB | Utilization  41%\n",
            "Run: 02, Epoch: 01, Loss: 3.9656, Train: 30.49%, Valid: 31.84% Test: 29.77%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Run: 02, Epoch: 02, Loss: 2.9254, Train: 33.04%, Valid: 34.74% Test: 33.10%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Run: 02, Epoch: 03, Loss: 2.3635, Train: 35.58%, Valid: 37.99% Test: 42.09%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Run: 02, Epoch: 04, Loss: 1.9753, Train: 29.99%, Valid: 23.81% Test: 27.93%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8675MB / 15360MB | Utilization  42%\n",
            "Run: 02, Epoch: 05, Loss: 1.7793, Train: 28.41%, Valid: 22.42% Test: 26.27%\n",
            "Run 02:\n",
            "Highest Train: 35.58\n",
            "Highest Valid: 37.99\n",
            "Highest Test: 42.09\n",
            "  Final Train: 35.58\n",
            "  Final Valid: 37.99\n",
            "   Final Test: 42.09\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8671MB / 15360MB | Utilization  42%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8671MB / 15360MB | Utilization  42%\n",
            "Run: 03, Epoch: 01, Loss: 3.9121, Train: 20.13%, Valid: 16.36% Test: 21.23%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Run: 03, Epoch: 02, Loss: 2.7992, Train: 29.30%, Valid: 31.44% Test: 29.87%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Run: 03, Epoch: 03, Loss: 2.2618, Train: 30.98%, Valid: 34.52% Test: 35.83%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Run: 03, Epoch: 04, Loss: 1.9596, Train: 29.51%, Valid: 32.67% Test: 35.58%\n",
            "LR is 0.001\n",
            "Adam training\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Testing\n",
            "GPU 0 ... Mem Free: 8493MB / 15360MB | Utilization  43%\n",
            "Run: 03, Epoch: 05, Loss: 1.7795, Train: 25.23%, Valid: 24.85% Test: 28.24%\n",
            "Run 03:\n",
            "Highest Train: 30.98\n",
            "Highest Valid: 34.52\n",
            "Highest Test: 35.83\n",
            "  Final Train: 30.98\n",
            "  Final Valid: 34.52\n",
            "   Final Test: 35.83\n",
            "All runs:\n",
            "Highest Train: 30.71 ± 5.02\n",
            "Highest Valid: 34.72 ± 3.18\n",
            "Highest Test: 37.31 ± 4.24\n",
            "  Final Train: 30.71 ± 5.02\n",
            "  Final Valid: 34.72 ± 3.18\n",
            "   Final Test: 37.31 ± 4.24\n"
          ]
        }
      ],
      "source": [
        "lr = 0.001\n",
        "optim_name = 'Adam'\n",
        "hidden_channels = 512\n",
        "use_SVI = False\n",
        "FC = False  \n",
        "change_ratio = False  \n",
        "ratio_mult = 0.25 if change_ratio else 1\n",
        "if __name__ == \"__main__\":  \n",
        "    result_dict = {'SVI-SGD': [], 'SVI-Adam': [], 'SGD': [], 'Adam': []}\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='OGBN-Arxiv (GNN)')\n",
        "    parser.add_argument('--log_steps', type=int, default=1)\n",
        "    parser.add_argument('--num_layers', type=int, default=4)\n",
        "    parser.add_argument('--dropout', type=float, default=0.25)\n",
        "    parser.add_argument('--lr', type=float, default=lr)\n",
        "    parser.add_argument('--momentum', type=float, default=0.95)\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--batch', type=int, default=1)\n",
        "    parser.add_argument('--runs', type=int, default=3)\n",
        "    parser.add_argument('--SVI', type=bool, default=use_SVI)\n",
        "    parser.add_argument(\n",
        "        '--optimizer', type=str, default=optim_name)\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()\n",
        "    args.FC = FC # If use fully-connected nets instead of GCN layers\n",
        "    args.change_ratio = change_ratio # If we change ratio of training and test (only used if \"ratio_mult\" < 1)\n",
        "    args.ratio_mult = ratio_mult  # Only use X% of training data\n",
        "    args.lr_drop = 0.98\n",
        "    args.dec_epoch = 100\n",
        "    if 1e-3 < args.lr and args.lr <= 1e-2:\n",
        "        args.lr_drop = 0.96\n",
        "    # args.hidden_channels = 512 if args.num_layers >= 3 else 1000\n",
        "    args.hidden_channels = hidden_channels\n",
        "    print(args)\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
        "                                    transform=T.ToSparseTensor())\n",
        "    data = dataset[0]\n",
        "    data.adj_t = data.adj_t.to_symmetric()\n",
        "    data = data.to(device)\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    # Split train to train sub and valid\n",
        "    # Always do because we do not want to \"peak\" into test features\n",
        "    train_full = split_idx['train']\n",
        "    N = len(train_full)\n",
        "    np.random.seed(1103)\n",
        "    rand_idx = np.sort(np.random.choice(N, int(N*args.ratio_mult), replace=False))\n",
        "    split_idx['train'] = split_idx['train'][rand_idx]\n",
        "    data_train=slide_idx(data, split_idx['train'])\n",
        "    logger = Logger(args.runs, args)\n",
        "    results_over_runs = {}\n",
        "    for run in range(args.runs):\n",
        "        accu_at_run = []\n",
        "        args.SVI = use_SVI\n",
        "        torch.manual_seed(1103 + run)\n",
        "        model = GNN(data.num_features, args.hidden_channels,\n",
        "                    dataset.num_classes, args.num_layers,\n",
        "                    args.dropout, args.FC).to(device)\n",
        "        evaluator = Evaluator(name='ogbn-arxiv')\n",
        "        if args.optimizer == 'SGD':\n",
        "            optimizer = torch.optim.SGD(\n",
        "                model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
        "        else:\n",
        "            optimizer = torch.optim.Adam(\n",
        "                model.parameters(), lr=args.lr)\n",
        "        for epoch in range(1, 1 + args.epochs):\n",
        "            if device.type == 'cuda':\n",
        "                # Useful to avoid GPU allocation excess\n",
        "                torch.cuda.empty_cache()\n",
        "            print(f\"LR is {optimizer.param_groups[0]['lr']}\")\n",
        "            optimizer.zero_grad()\n",
        "            # epoch_stop_SVI = 50\n",
        "            epoch_stop_SVI = 1000\n",
        "            if epoch == epoch_stop_SVI + 1 and args.SVI:\n",
        "                # Reinitialize optimizer to avoid gradient issue\n",
        "                args.SVI = False\n",
        "                sdict = model.state_dict()\n",
        "                print(\n",
        "                    '############ Pause SVI from now on ############')\n",
        "                model = GNN(data.num_features, args.hidden_channels,\n",
        "                    dataset.num_classes, args.num_layers,\n",
        "                    args.dropout, args.FC).to(device)\n",
        "                model.load_state_dict(sdict)\n",
        "                model = model.to(device)\n",
        "                optimizer = torch.optim.Adam(\n",
        "                    model.parameters(), lr=args.lr)\n",
        "            if args.SVI:\n",
        "                print(f'SVI-{args.optimizer} training')\n",
        "                loss = train_SVI(model, data_train, args)\n",
        "            else:\n",
        "                print(f'{args.optimizer} training')\n",
        "                loss = train(model, data_train, args)\n",
        "            optimizer.step()\n",
        "            mem_report()\n",
        "            # if epoch > args.dec_epoch:\n",
        "            #     for p in optimizer.param_groups:\n",
        "            #         p['lr'] *= args.lr_drop\n",
        "            print('Testing')\n",
        "            result = test(model, data_train, data, split_idx, evaluator)\n",
        "            mem_report()\n",
        "            logger.add_result(run, result)\n",
        "            if epoch % args.log_steps == 0:\n",
        "                train_acc, valid_acc, test_acc = result\n",
        "                accu_at_run += [[train_acc, valid_acc, test_acc]]\n",
        "                print(f'Run: {run + 1:02d}, '\n",
        "                      f'Epoch: {epoch:02d}, '\n",
        "                      f'Loss: {loss:.4f}, '\n",
        "                      f'Train: {100 * train_acc:.2f}%, '\n",
        "                      f'Valid: {100 * valid_acc:.2f}% '\n",
        "                      f'Test: {100 * test_acc:.2f}%')\n",
        "        # Running np.array(accu_at_run) would make it into Epoch-by-3 matrices, but doing so causes .json saving error so I just use the list version\n",
        "        results_over_runs[f'lr={args.lr}@Run{run+1}'] = accu_at_run\n",
        "        logger.print_statistics(run)\n",
        "        # Save results\n",
        "        key = f'SVI-{optim_name}' if use_SVI else optim_name \n",
        "        fc_use = '-FC' if args.FC else ''\n",
        "        c_ratio = '-change_ratio' if args.change_ratio else ''\n",
        "        ratio = args.ratio_mult if args.ratio_mult < 1 else ''\n",
        "        key_save = f'SVI-{optim_name}-{args.num_layers}layers-{args.hidden_channels}nodes-{args.lr}LR{fc_use}{c_ratio}{ratio}_correct_split' if use_SVI else f'{optim_name}-{args.num_layers}layers-{args.hidden_channels}nodes-{args.lr}LR{fc_use}{c_ratio}{ratio}_correct_split'\n",
        "        logger.pickle(key_save) # Save it to file, but need not now because only one run.\n",
        "        result_dict[key].append(results_over_runs)\n",
        "        with open(f\"{key_save}.json\", \"w\") as outfile:\n",
        "            json.dump(result_dict, outfile)\n",
        "    logger.print_statistics()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}